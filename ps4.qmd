---
title: "problem set 4"
format: html
editor: visual
---

```{r message=FALSE}
library(tidyverse)
library(haven)
library(estimatr)
library(fixest)
library(broom)
library(purrr)
library(plm)
library(future)
library(future.apply)

# parallel processing thingy
plan(multisession)

# import data
usdot = read_csv('https://ditraglia.com/data/usdot.csv')
cardkreuger = read_dta('https://ditraglia.com/data/minwage.dta')
minwage = read_dta('https://ditraglia.com/data/minwage.dta')

# set parameters
n = 30
alpha = 0
beta = 0
seed = 42069
set.seed(seed)
```

# Airfare

## 1.
```{r}
# create time dummies and log variables
usdot = usdot |>
  # i didn't know there was a shortcut to regress with dummies 
  mutate(year_1998 = if_else(year == 1998, 1, 0), 
         year_1999 = if_else(year == 1999, 1, 0), 
         year_2000 = if_else(year == 2000, 1, 0),
         logdist = log(distance), 
         logdist_2 = logdist^2, 
         logairfare = log(airfare))

# regress log(airfare) on market_share, log(dist), log(dist)^2, year dummies
model = lm(logairfare ~ market_share + logdist + logdist_2 + year_1998 + 
             year_1999 + year_2000, data=usdot)
```

### a)
Time dummies capture unobserved factors that vary over time but are constant across routes so that we can allow for the common time-varying (unobserved) component of the error term, like inflation or something. The ones for 1999 and 2000 are statisticall significant at the 5% level, and the estimated coefficients get larger for each passing year. Since the dataset ends in 2000, it makes sense for them all to be positive and also increasing since with inflation the prices will increase a little bit each year.

```{r}
tidy(model)
```

### b)
A one percentage point increase in the largest airline that has a particular route is associated with a 0.36% increase in the price of the one-way ticket. That is like the largest airline dominating the market a little more so they have more market power. The 95% confidence interval is given by
```{r}
confint(model, parm='market_share')
```

We are 95% confident that the true effect of a 1 percentage point increase in market share is within 0.30 and 0.42.

### c)
Clustering by route_id because it's like the only group variable we have?
```{r}
model_cluster = lm_robust(logairfare ~ market_share + logdist + logdist_2 + 
                            year_1998 + year_1999 + year_2000,
                          data=usdot,
                          clusters=route_id)

tidy(model_cluster)
```

These cluster-robust standard errors are larger.

## 2.

### a)
We have $\frac{d\log(Y)}{dY} = \frac{1}{Y}$ so if you rearrange it you get $d\log(Y) = \frac{dY}{Y}$, or $d\log(X) = \frac{dX}{X}$ for $X$. Then, substitute these and get that the elasticity of $Y$ w/rt. $X$ is given by $\frac{dY/Y}{dX/X} = \frac{d\log(Y)}{d\log(X)}$.

### b)
Because of the specification for this regression, $\frac{d\log(Y)}{d\log(X)}$ is given by $\hat{\beta}_{logdist} + 2\hat{\beta}_{logdist^2}$

### c)
```{r}
# estimated coefficients
beta_logdist = coef(model_cluster)['logdist']
beta_logdist_2 = coef(model_cluster)['logdist_2']  

# calculate the elasticities
usdot = usdot |>
  mutate(elasticity = beta_logdist + 2 * logdist * beta_logdist_2)
```

Plotting elasticity and distances:
```{r}
ggplot(usdot, aes(x = distance, y = elasticity)) +
  geom_point()
```

### d)
The figure shows that as the distance of a particular route increases, the elasticity of price with respect to distance increases. A 1% increase in distance of a route leads to a larger percentage increase in price when the route is longer. I think this makes sense since the longer the flight, the fewer airlines are likely to offer it. For example, the current world's longest flight is only offered by Singapore Airlines. If there are fewer competing airlines for longer-distance routes, then an airline can raise prices more.

## 3.

### a)
Fixed effects vary across alternatives but not across time, which is trying to capture the other component of the error term, the individual effect. I think this makes sense to add because different routes have their own characteristics that aren't captured by the variables we have in the data, like region-specific weather patterns and how boring a city is.

### b)
$\log{airfare_{i,t}} = \alpha_{i} + market\_share_{i,t} + \log{dist_{i}} + (\log{dist_{i}})^2 + (1998 == 1)_{t} + (1999 == 1)_{t} + (2000 == 1)_{t}$

We cannot add time-invariant regressors because they are perfectly collinear with the dummies, meaning that distance can't be added as a regressor. This is because distance is constant for all observations within the same route and so it will always be the same multiple of the corresponding dummy variable for that route. However, that means we can't find $\hat{\beta}_{logdist} + 2\hat{\beta}_{logdist^2}$ which is the elasticity that we derived in question 2.

### c)
```{r}
# but remove the dist variables bc they are perfectly collinear with the dummies
model_fe = feols(logairfare ~ market_share + year_1998 + year_1999 + 
                   year_2000 | route_id, 
                 data = usdot,
                 cluster=~route_id)

tidy(model_fe)
```
Now, a one percentage point increase in the largest airline that has a particular route is associated with a 0.16% increase in the price of the one-way ticket when you have include route dummies. I think it makes sense for it to be lower because now we're controlling for routes with higher market_share that also have unobserved features causing higher prices. For example, a route served by a very large airline might also be more popular for business travel. The time dummies are similar in both regressions which is probably just because inflation is just doing its own thing. We also don't have any distance effects, but I think if you're controlling for route-specific things anyways then you don't need to add distance by itself anyways.

## 4.

### a) 
Tbh I don't really understand this question because if you can't reasonably assume that the individual specific error is uncorrelated with the explanatory variables then you don't even get a consistent estimator anyways so there's no benefit of one or the other just on their own---it depends on if you can assume one or the other in the first place. I thought the two were just an assumption, not actually the model itself? I think if you add the average across time then that variable is correlated with the the unobserved individual-specific error component since it's also individual specific.

### b)
You can add the individual-specific distance variables here since there won't be perfect multicollinearity, so you can find the elasticity.

$\log{airfare_{i,t}} = \beta_{0} + mean\_market\_share_{i} + market\_share_{i,t} + \log{dist_{i}} + (\log{dist_{i}})^2 + (1998 == 1)_{t} + (1999 == 1)_{t} + (2000 == 1)_{t}$

### c)
```{r}
# route averages
usdot = usdot |>
  group_by(route_id) |>
  mutate(market_share_route=mean(market_share))

# estimate the model
random_effects = plm(logairfare ~ market_share_route + market_share + 
                       logdist + logdist_2 + year_1998 + year_1999 + year_2000, 
                     data=usdot)

tidy(random_effects)
```

### d)
The estimated coefficient on market_share is the same as in the fixed effects model and the coefficient on the average market_share variable is significant at the 5% level but the standard errors are larger. If there are no individual-specific effects (no serial correlation) in the error term, the random effects GLS and OLS estimators coincide. Pooled OLS is efficient under the one-way error components model (thank u steve bond a true diva)

# Behrens-Fisher

## 1.
This is not balanced since much more observations will have $D=0$ than $D=1$.
```{r}
n1 = 3
n0 = n - n1
sigma = 0.5

# generate dummy variables according to n1 and n - n1
D = c(rep(1, n1), rep(0,n0)) 

# generating the u_is with the heteroskedasticity
u = rnorm(n, mean=0, sd=ifelse(D == 1, 1, sigma))

# generating Y using alpha, beta, and u
Y = alpha + beta * D + u
```


## 2.
```{r}
# get group means of Y
Y_1_bar = mean(Y[D == 1])
Y_0_bar = mean(Y[D == 0])

# get ols estimator
beta_hat = Y_1_bar - Y_0_bar

# residual sums of squares
RSS_0 = sum((Y[D == 0] - Y_0_bar)^2)
RSS_1 = sum((Y[D == 1] - Y_1_bar)^2)

# the various flavors of “robust” standard errors

# regular
se_usual = sqrt((n / (n0 * n1)) * ((RSS_0 + RSS_1) / (n - 2)))
print(paste0('non-robust standard error: ', se_usual))

# HC0
se_0 = sqrt((RSS_0 / n0^2) + (RSS_1 / n1^2)) 
print(paste0('HC0: ', se_0))

# HC1
se_1 = sqrt((n / (n - 2)) * (RSS_0 / n0^2 + RSS_1 / n1^2))
print(paste0('HC1: ', se_1))

# HC2
se_2 = sqrt((RSS_0 / (n0 * (n0 - 1))) + (RSS_1 / (n1 * (n1 - 1)))) 
print(paste0('HC2: ', se_2))

# HC3
se_3 = sqrt((RSS_0 / (n0 - 1)^2) + (RSS_1 / (n1 - 1)^2))  
print(paste0('HC3: ', se_3))
```
Check these with the lm and lm_robust output:
```{r}
# run OLS
model_sim = lm(Y ~ D)
model_sim_robust = lm_robust(Y ~ D)

# non-robust standard error
summary(model_sim)$coefficients['D', 'Std. Error']

# robust standard errors
model_sim_HC0 = lm_robust(Y ~ D, se_type='HC0')
model_sim_HC0$std.error['D']

model_sim_HC1 = lm_robust(Y ~ D, se_type='HC1')
model_sim_HC1$std.error['D']

model_sim_HC2 = lm_robust(Y ~ D, se_type='HC2')
model_sim_HC2$std.error['D']

model_sim_HC3 = lm_robust(Y ~ D, se_type='HC3')
model_sim_HC3$std.error['D']
```

## 3.
I think having $\sigma = 0.5$ is more heteroskedasticity than $\sigma = 1$ since when $\sigma = 1$, the variance of $u_i$ is always 1 and that's homoskedastic, but for $\sigma = 0.5$ the variance of u_i varies with the value of $D_i$.

Functions:
```{r}
# with locally defined parameters
draw_sim_data = function(sigma, n1, n=30, alpha=0, beta=0) {
  
  # define n0
  n0 = n - n1
  
  # generate dummy variables according to n1 and n - n1
  D = c(rep(1, n1), rep(0, n0)) 
  
  # generating the u_is with the heteroskedasticity
  u = rnorm(n, mean=0, sd=ifelse(D == 1, 1, sigma))
  
  # generating Y using alpha, beta, and u
  Y = alpha + beta * D + u
  
  return(tibble(sim_Y = Y, 
                sim_D = D, 
                sim_u = u))
}

compute_SEs = function(sim_data) {
  # redefine all these things again
  Y = sim_data$sim_Y
  D = sim_data$sim_D
  n = nrow(sim_data)
  n1 = length(filter(sim_data, sim_data$sim_D == 1))
  n0 = n - n1
  
  # get group means of Y
  Y_1_bar = mean(Y[D == 1])
  Y_0_bar = mean(Y[D == 0])
  
  # get ols estimator
  beta_hat = Y_1_bar - Y_0_bar
  
  # residual sums of squares
  RSS_0 = sum((Y[D == 0] - Y_0_bar)^2)
  RSS_1 = sum((Y[D == 1] - Y_1_bar)^2)
  
  # the various flavors of “robust” standard errors
  
  # regular
  se_usual = sqrt((n / (n0 * n1)) * ((RSS_0 + RSS_1) / (n - 2)))
  
  # HC0
  se_0 = sqrt((RSS_0 / n0^2) + (RSS_1 / n1^2)) 
  
  # HC1
  se_1 = sqrt((n / (n - 2)) * (RSS_0 / n0^2 + RSS_1 / n1^2))
  
  # HC2
  se_2 = sqrt((RSS_0 / (n0 * (n0 - 1))) + (RSS_1 / (n1 * (n1 - 1)))) 
  
  # HC3
  se_3 = sqrt((RSS_0 / (n0 - 1)^2) + (RSS_1 / (n1 - 1)^2))  
  
  # return all the standard errors and stuff 
  return(list(beta_hat = beta_hat,
              se_nonrobust = se_usual, 
              se_HC0 = se_0,
              se_HC1 = se_1,
              se_HC2 = se_2,
              se_HC3 = se_3))
}

get_sim_results = function(sigma, n1, n_reps=1e5) {
  
  # store results to a dataframe
  sims = tibble(beta_hat=numeric(), 
                se_nonrobust=numeric(), 
                se_HC0=numeric(), 
                se_HC1=numeric(),
                se_HC2=numeric(),
                se_HC3=numeric())
  
  # repeat draws
  for (rep in 1:n_reps) {
    # call draw_sim_data to get each draw
    draws = draw_sim_data(sigma=sigma, n1=n1)
    
    # call compute_SEs to get all the estimates
    estimates = compute_SEs(draws)
    
    # append results to a dataframe by creating a new row
    new_row = tibble(beta_hat=estimates$beta_hat, 
                     se_nonrobust=estimates$se_nonrobust, 
                     se_HC0=estimates$se_HC0, 
                     se_HC1=estimates$se_HC1, 
                     se_HC2=estimates$se_HC2, 
                     se_HC3=estimates$se_HC3)
    
    # and attaching them
    sims = bind_rows(sims, new_row)    
  }
  
  # return results
  return(sims)
}

# this is the future one which i don't totally get tbh
get_sim_results_p = function(sigma, n1, n_reps=1e5) {
  # futurise the replications
  results=future_lapply(1:n_reps, function(rep) {
    draws=draw_sim_data(sigma=sigma, n1=n1)
    compute_SEs(draws)}, 
    future.seed = TRUE)
  
  # convert the parallelised stuff to tibble
  sims = map_dfr(results, 
                 ~ tibble(beta_hat = .x$beta_hat,
                          se_nonrobust = .x$se_nonrobust,
                          se_HC0 = .x$se_HC0,
                          se_HC1 = .x$se_HC1,
                          se_HC2 = .x$se_HC2,
                          se_HC3 = .x$se_HC3))
  
  return(sims)
  return(sims)
}

# sampling distribution, rejection rates
samplingdist_rejections_sims = function(sims) {
  # constructing the t statistic and such
  sims = sims |>
    # i keep forgetting you can have multiple arguments in mutate
    mutate(t_nonrobust = beta_hat / se_nonrobust, # the t statistics
           t_HC0 = beta_hat / se_HC0,
           t_HC1 = beta_hat / se_HC1,
           t_HC2 = beta_hat / se_HC2,
           t_HC3 = beta_hat / se_HC3,
           # the rejections
           reject_nonrobust = ifelse(abs(t_nonrobust) > 1.96, 1, 0),
           reject_HC0 = ifelse(abs(t_HC0) > 1.96, 1, 0), 
           reject_HC1 = ifelse(abs(t_HC1) > 1.96, 1, 0), 
           reject_HC2 = ifelse(abs(t_HC2) > 1.96, 1, 0), 
           reject_HC3 = ifelse(abs(t_HC3) > 1.96, 1, 0))
  
  # sampling distributions of the estimated values
  for (i in c('beta_hat', 'se_nonrobust', 'se_HC0', 
              'se_HC1', 'se_HC2', 'se_HC3')) {
    # get mean and true standard error
    print(paste0('mean (', i, '): ', mean(coconutshavewaterinthem[[i]])))
    print(paste0('se (', i, '): ', sd(coconutshavewaterinthem[[i]])))
  }
  
  # rejection rates for the t statistic
  print(sims |>
    summarise(rate_nonrobust = mean(reject_nonrobust), 
              reject_rate_HC0 = mean(reject_HC0),
              reject_rate_HC1 = mean(reject_HC1),
              reject_rate_HC2 = mean(reject_HC2),
              reject_rate_HC3 = mean(reject_HC3)))
  
  return(sims)
}
```

Getting the data:
```{r}
sim1 = get_sim_results_p(sigma=0.5, n1 = 3)
```

The sampling distribution of $\hat{\beta}$:
```{r}
sim1 = samplingdist_rejections_sims(sim1)
```

## 4.

### Part 1
Getting the data:
```{r}
sim2 = get_sim_results_p(sigma=0.85, n1 = 3)
```

The sampling distribution of $\hat{\beta}$:
```{r}
sim2 = samplingdist_rejections_sims(sim2)
```
I think having $\sigma = 0.85$ is less heteroskedasticity than $\sigma = 0.5$ since when $\sigma = 0.5$, the variance of $u_i$ is either $1$ or $0.25$, which are not as close together as $\sigma = 0.85$, where the variance of u_i varies with the value of $D_i$ but varies between $\sqrt{0.85}$ and $1$, which are closer together.

### Part 2
Getting the data:
```{r}
sim3 = get_sim_results_p(sigma=1, n1 = 3)
```

The sampling distribution of $\hat{\beta}$:
```{r}
sim3 = samplingdist_rejections_sims(sim3)
```

This is the one that doesn't have heteroskedasticity since the variance is always constant and therefore doesn't vary with $D_i$.

## 5.
Merging the data:
```{r}
combined_results = bind_rows(sim1 |> mutate(sigma = 0.5),
                             sim2 |> mutate(sigma = 0.85),
                             sim3 |> mutate(sigma = 1))
```

Plotting:
```{r}
# plot density by sigma (colours applied by sigma)
ggplot(data=combined_results, aes(x=beta_hat, group=sigma, 
                                  fill=factor(sigma))) +
  # this is because i am slightly blind
  coord_cartesian(xlim = c(-1, 1)) +
  # i don't really understand what this does but it's in the documentation
  geom_density(adjust=1.5, alpha=.4)
```

## 6.
The sample distributions looks very similar. It's not necessarily that the one with $\sigma=0.5$ is more right skewed than the one with $\sigma=0.85$, it's more like it just has a different shape than the other two. However, both show finite sample bias compared to the no-heteroskedasticity simulation There are few, only three, "treated" observations $D_i = 1$ in all three simulations and these guys all have a higher variance than the untreated observations. In the $\sigma=0.5$ simulation, the rejection rate is higher when using the non-robust standard error, but in the $\sigma=0.85$ and $\sigma=1$ simulations the opposite is true.

# Minimum Wage

## 1.
```{r}
minwage = minwage |>
  # restrict sample
  filter(sample == 1) |>
  # rename treatment
  rename(treat=state) |>
  mutate(state=ifelse(treat == 1, 'NJ', 'PA')) |>
  # low wage dummy
  mutate(lowwage=ifelse(wage_st < 5, 1, 0))
```

## 2.

### a)
```{r}
# wage means by state and wave

# first wave

# NJ
avg_wave_st_NJ = minwage |> 
  filter(treat == 1) |> 
  summarise(mean_wage=mean(wage_st, na.rm=TRUE)) |> 
  pull(mean_wage)

# PA
avg_wave_st_PA = minwage |> 
  filter(treat == 0) |> 
  summarise(mean_wage=mean(wage_st, na.rm=TRUE)) |> 
  pull(mean_wage)

# second wave

# NJ
avg_wave_st2_NJ = minwage |> 
  filter(treat == 1) |> 
  summarise(mean_wage=mean(wage_st2, na.rm=TRUE)) |> 
  pull(mean_wage)

# PA
avg_wave_st2_PA = minwage |> 
  filter(treat == 0) |> 
  summarise(mean_wage=mean(wage_st2, na.rm=TRUE)) |> 
  pull(mean_wage)

print(paste0('average wave 1 starting wage in NJ: ', avg_wave_st_NJ))
print(paste0('average wave 2 starting wage in NJ: ', avg_wave_st2_NJ))
print(paste0('average wave 1 starting wage in PA: ', avg_wave_st_PA))
print(paste0('average wave 2 starting wage in PA: ', avg_wave_st2_PA))
```

### b)
```{r}
# time differences

# NJ
d_wave_st_NJ = avg_wave_st2_NJ - avg_wave_st_NJ

# PA
d_wave_st_PA = avg_wave_st2_PA - avg_wave_st_PA

print(paste0('difference in mean starting wages in NJ: ', d_wave_st_NJ))
print(paste0('difference in mean starting wages in PA: ', d_wave_st_PA))
```

### c)
```{r}
# state difference
dd_wave_st = d_wave_st_NJ - d_wave_st_PA

print(paste0('between-state difference-in-differences: ', dd_wave_st))
```

### d)
We see that the difference in mean starting wages in the treated state NJ between wave 1 and wave 2 (before and after the increase in minimum wage) is positive. There is a higher starting wage for NJ in wave 2. For the untreated state PA, there is little difference in mean starting wages between the two waves (with no change in minimum wage in between). The differences in these differences is positive, in this case meaning that the treatment state saw a larger increase in mean starting wage in response to the increase in minimum wage. We assume no anticipation effects (employers don't prematurely change starting wages because they know the legislation is changing soon) and no trend (if there was no change in minimum wage, the average starting wage in NJ would not change over the two waves). Then, it appears that the minimum wage increase caused the increase in mean starting wage in NJ, which makes sense since you're raising the minimum for everyone. It would not make sense for employers hiring for low wage positions to "unnecessarily" increase wages in anticipation of this change since the labour would cost them more. Since the two waves occur in the same year, I think it makes sense for the no trend assumption to hold unless there was some kind of big event or short-term inflation or something that I don't know about.

## 3.

### a)
```{r}
# wage means by state and wave

# first wave

# NJ (grab only treat = 1 observations and take that as a variable)
avg_fte_NJ = minwage |> 
  filter(treat == 1) |> 
  summarise(mean_emp=mean(fte, na.rm=TRUE)) |> 
  pull(mean_emp)

# PA
avg_fte_PA = minwage |> 
  filter(treat == 0) |> 
  summarise(mean_emp=mean(fte, na.rm=TRUE)) |> 
  pull(mean_emp)

# second wave

# NJ
avg_fte2_NJ = minwage |> 
  filter(treat == 1) |> 
  summarise(mean_emp=mean(fte2, na.rm=TRUE)) |> 
  pull(mean_emp)

# PA
avg_fte2_PA = minwage |> 
  filter(treat == 0) |> 
  summarise(mean_emp=mean(fte2, na.rm=TRUE)) |> 
  pull(mean_emp)

print(paste0('average wave 1 full time equivalent employment in NJ: ', 
             avg_fte_NJ))
print(paste0('average wave 2 full time equivalent employment in NJ: ', 
             avg_fte2_NJ))
print(paste0('average wave 1 full time equivalent employment in PA: ', 
             avg_fte_PA))
print(paste0('average wave 2 full time equivalent employment in PA: ', 
             avg_fte2_PA))
```
### b)
```{r}
# time differences

# NJ
d_fte_NJ = avg_fte2_NJ - avg_fte_NJ

# PA
d_fte_PA = avg_fte2_PA - avg_fte_PA

print(paste0('difference in mean full time equivalent employment in NJ: ', 
             d_fte_NJ))
print(paste0('difference in mean full time equivalent employment in PA: ', 
             d_fte_PA))
```

### c)
```{r}
# state difference
dd_fte = d_fte_NJ - d_fte_PA

print(paste0('between-state difference-in-differences: ', dd_fte))
```

### d)
We see that there is little difference in mean full time equivalent employment in the treated state NJ between wave 1 and wave 2 (before and after the increase in minimum wage). For the untreated state PA, mean full time equivalent employment decreases between wave 1 and wave 2. The differences in these differences is positive, in this case meaning that the treated state saw less of a decrease in mean full time equivalent employment in response to the increase in minimum wage. We assume no anticipation effects (employers don't prematurely change hiring because they know the legislation is changing soon) and no trend (if there was no change in minimum wage, the average employment in NJ would not change over the two waves). This effect could be because workers from PA migrate over to NJ to take advantage of the higher minimum wage. However, this would likely violate the no anticipation effects assumption since these changes are usually announced beforehand and anyone who notices it in the news might consider moving.

## 4.
```{r}
# take wave 1 data
wave1 = minwage[c('state', 'wage_st', 'fte', 'chain', 
                 'co_owned', 'sheet', 'treat', 'lowwage')] |>
  # add the time dummy
  mutate(post = 0) |>
  rename(restaurant_id = sheet)

# take wave 2 data
wave2 = minwage[c('state', 'wage_st2', 'fte2', 'chain', 
                 'co_owned', 'sheet', 'treat', 'lowwage')] |>
  # add the time dummy
  mutate(post = 1) |>
  # rename things to make them readable/make the merge happen
  rename(wage_st = wage_st2, 
         fte = fte2, 
         restaurant_id = sheet)

# concat to make one long dataframe
both_waves = bind_rows(wave1, wave2)
```

## 5.

### a)
$\beta_{0}$: value of the outcome variable for the untreated group in the first survey wave
$\beta_{1}$: difference in the outcome variable for the treated and untreated groups in the first survey wave
$\beta_{2}$: difference in the outcome variable for the untreated groups between the first and second survey wave. This is the time difference for the untreated group like in the p. Similarly, the time difference for the treated group is given by $\beta_{2} + \beta_{3}$. This means that the difference-in-differences is given by $\beta_{3}$.

### b)
I clustered by chain because maybe different restaurant chains have some unique characteristics like different employee transfer rules or different hiring practices. The estimated differences-in-differences effect is the same the results in part 2. I can also see that the coefficient on `post` is not significantly different from 0, while the differences-in-differences estimate is.
```{r}
# outcome variable wage_st + interaction term
model_dd_wage_st = lm_robust(wage_st ~ treat * post, 
                     data=both_waves,
                     clusters=chain)

summary(model_dd_wage_st)
```

### c)
The estimated differences-in-differences effect is also the same here as the results in part 3. Here, there are no statistically significant coefficients aside from the intercept.
```{r}
# outcome variable fte + interaction term
model_dd_fte = lm_robust(fte ~ treat * post, 
                     data=both_waves,
                     clusters=chain)

summary(model_dd_fte)
```

### d)
```{r}
# outcome variable wage_st with clustering by chain and dummies and co_owned
model_dd_wage_st_c = lm_robust(wage_st ~ co_owned + 
                                 treat * post + factor(chain), 
                     data=both_waves,
                     clusters=chain)

# outcome variable fte with clustering by chain and dummies and co_owned
model_dd_fte_c = lm_robust(fte ~ co_owned + 
                             treat * post + factor(chain), 
                     data=both_waves,
                     clusters=chain)

summary(model_dd_wage_st_c)
summary(model_dd_fte_c)
```

### e)
The differences-in-differences estimates are the same as in the previous parts. I think this is because we added some things that aren't time varying so then you can't control for time-related stuff with those.

## 6.

### a)
I don't exactly understand why this question is different from the one in question 2 but in terms of the regression itself we need there to be no other state-specific events that only impact one of the states but not the other, which seems to be reasonable since the states are really close to each other and chain restaurants are generally supposed to be similar from location to location. Also, I think we need the same restaurants to be represented in both waves (like if one chain closes a lot of locations due to the change, then I don't think you can say that you're estimating the true effect of raising minimum wage).

### b)
The time difference for the low wage group is given by $\hat{\beta_{2}} + \hat{\beta_{3}} = -0.004091 + 0.615872$. These wages increased. The time difference for the high wage group is given by $\hat{\beta_{2}} = -0.004091$, which we don't find to be significantly different from zero. The differences-in-differences estimate says that the starting wages increased more for the low wage restaurants than the high wage restaurants.
```{r}
# get NJ data
both_waves_NJ = both_waves |>
  filter(treat == 1)

# run regression on low wage vs non-low wage in NJ only
model_NJ = lm_robust(wage_st ~ lowwage * post, 
                     data=both_waves_NJ,
                     clusters=chain)

summary(model_NJ)
```

### c)
For this regression we need the low-wage and high-wage NJ restaurants to follow the same wage setting trends in the absence of the minimum wage law. This is probably not realistic since low and high starting wages may reflect differences in the work tasks required in different restaurants or different locations (like cities with higher costs of living).

### d)
The time difference for the low wage group is given by $\hat{\beta_{2}} + \hat{\beta_{3}} = -0.2652 + 0.3536$. These wages increased. The time difference for the high wage group is given by $\hat{\beta_{2}} = -0.2652$, which is actually significantly different from zero. Similarly to in NJ, the differences-in-differences estimate says that the starting wages increased more for the low wage restaurants than the high wage restaurants (but is lower than in NJ). This means there was probably a bit of wage adjustment in PA in response to the change in NJ because employers with low-wage employees in PA didn't want all their employees to move away for higher-paying jobs.
```{r}
# get PA data in a kind of complicated way i think
both_waves_PA = both_waves |>
  filter(treat == 0)

# run regression on low wage vs non-low wage in PA only
model_PA = lm_robust(wage_st ~ lowwage * post, 
                     data=both_waves_PA,
                     clusters=chain)

summary(model_PA)
```
