---
title: "Core ERM Problem Set 2"
author: '1088708'
date: "`r Sys.Date()`"
format: html
editor: visual
---

```{r message=FALSE}
library(scales)
library(tidyverse)
library(purrr)
library(EnvStats)
library(GGally)
library(stargazer)
library(haven)
library(modelsummary)
library(broom)

# import data
football = read_csv('https://ditraglia.com/data/fair_football.csv')
wells = read_csv('https://ditraglia.com/data/wells.csv')
nsw_dw = read_dta('https://users.nber.org/~rdehejia/data/nsw_dw.dta')
controls = read_dta('https://users.nber.org/~rdehejia/data/cps_controls.dta')

# set seed
seed = 42
set.seed(seed)

# set parameters
alpha = 0.05
a = 75
m = 2^16 + 1
computer_rankings = c('MAT', 'SAG', 'BIL', 'COL', 'MAS', 'DUN')

# function(s)

# calculating mean squared error using regression residuals
get_MSE = function(regression_output) {
  return(mean(residuals(regression_output)^2))
}


# wells partial effects and stuff
logit_effects = function(fit, predictor) {
  # grab the data that's still stored in fit
  data = model.frame(fit)
  
  # find the characteristics of the average person in the data
  mean_data = as.data.frame(lapply(data, function(x) mean(x, na.rm=TRUE)))
  
  # predicted probability of switch at the mean
  p_avg = predict.glm(fit, newdata=mean_data, type='response')[[1]]
  
  # density for partial effect
  d_avg = dlogis(predict(fit, newdata=mean_data)[[1]])
  
  # estimated logit coefficient of the predictor
  bhat = coef(fit)[[predictor]]
  
  # marginal effect at the average
  MEA = bhat * d_avg
  
  # divide-by-4 rule thing
  div4 = bhat / 4
  
  # predicted probability of switch for all observations
  p_all = predict.glm(fit, type='response')
  
  # density for partial effect
  d_all = dlogis(predict(fit))
  
  # average marginal effect
  APE = mean(bhat * d_all, na.rm=TRUE)
  
  # return everything
  return(list(
    mean_pred=mean_data[[predictor]],
    probability_at_mean=p_avg,
    coefficient=bhat,
    marginal_effect_average=MEA,
    divide_by_4=div4,
    predicted_probabilities=p_all,
    average_partial_effect=APE
  ))
}


# data cleaning for part 1 of NSW
cleanup = function(data) {
  data = data |>
    # b) rename earnings variables
    rename(earnings74 = re74) |>
    rename(earnings75 = re75) |>
    rename(earnings78 = re78) |> 
    
    # c) create string race variable
    mutate(race = case_when(
      black == 1 ~ 'black',
      hispanic == 1 ~ 'hispanic',
      TRUE ~ 'white')) |>
    
    # d) create string treat, nodegree, and marriage
    mutate(str_treat = ifelse(treat == 1, 'treated', 'untreated')) |>
    mutate(str_nodegree = ifelse(nodegree == 1, 'no degree', 'degree')) |>
    mutate(str_married = ifelse(married == 1, 'married', 'unmarried')) |>
    
    # e) create employment variables
    mutate(employment74 = ifelse(earnings74 != 0, 1, 0)) |>
    mutate(employment75 = ifelse(earnings75 != 0, 1, 0)) |>
    # create string employment variables
    mutate(str_employment74 = ifelse(employment74 == 1, 'employed', 
                                     'unemployed')) |>
    mutate(str_employment75 = ifelse(employment75 == 1, 'employed', 
                                     'unemployed'))
  
  # f) why would you drop variables you're going to use later
  # drop data_id
  data = data[, !(names(data) == 'data_id')]
  
  return(data)
}

```

# Monte Carlo

## 1.

### a)

slayyyyyyy $\displaystyle X_{k+1}=a\cdot X_{k}{\bmod {m}}$

```{r}
# print next element
# so X_0 is the seed?
X1 = (a*seed) %% m
print (X1)
```

### b)

```{r}
# rescale the thing

Lehmer_seq <- c(42, 3150, 39639, 23760, 12501, 20057)

# scaled to [0,1]
Lehmer_seq_norm = rescale(Lehmer_seq, to=c(0, 1))

# scaled to [3,5]
Lehmer_seq_norm3 = rescale(Lehmer_seq, to=c(3, 5))
```

### c)

```{r}
# simulate standard (independent) uniform draws
runif_zx81 <- function(seed, n, a=75, m=2^16 + 1, min=0, max=1) {
  
  # Add warning messages in case the seed input is negative or larger than m.
  if ((seed < 0 )| (seed > m)) {
    warning('uh yeah i sure hope it does')
  }
  
  # store the numbers
  draws = c(seed)
  # draw n numbers (including the first number)
  for (i in 1:(n-1)) {
    # find the most recent number
    last_draw = draws[length(draws)]
    # get the next number
    next_lehmer = (a*last_draw) %% m
    # add it to the vector
    draws = c(draws, next_lehmer)
  }
  
  # first check if you've got the right package
  if (!requireNamespace('scales', quietly = TRUE)) {
    stop('road work ahead')
  }
  # then scale the generated numbers
  draws = scales::rescale(draws, to=c(0, 1))
  
  # listen i don't care if people think it's stupid
  # functions should actually tell you what they're returning
  return (draws)

}
```

### d)

```{r}
# 1000 numbers
draw1000 = runif_zx81(seed=42, n=1000)

# get a tibble
draw1000_df = mutate(tibble(draw1000), n = row_number())
```

View a histogram: looks pretty uniform since the frequencies are pretty similar for all the possible value.

```{r}
hist(draw1000)
```

View a QQ plot: I think if it lines up with the 45-degree line it's supposed to match the distribution set in the parameters so this one looks fine

```{r}
# this one is from envstats
qqPlot(draw1000, distribution='unif', param.list=list(min=0, max=1))
```

View a time series plot: all values are between 0 and 1

```{r}
ggplot(draw1000_df, aes(x=n, y=draw1000)) +
  geom_line()
```

These all suggest a uniform distribution.

## 2.

### a)

```{r, tidy=FALSE, eval=FALSE, highlight=FALSE}

rnorm_zx81 = function(seed, n, mean, sd) {
  
  # get two standard uniform vectors using runif_zx81()
  # just run it once with n/2 draws and split it in half
  # or (n+1)/2 draws if n is odd
  
  # do the box transform to get R and theta
  
  # use R and theta to get Z1 and Z2
  
  # combine z1 and z2 into one vector and return it
  # or return the first n elements if n is odd
}

```

### b)

```{r}
# have to assume these are iid
unif_seq <- c(0.5600805, 0.5767570, 0.8858708, 0.9313472, 0.7665961, 0.9763004)

mean = 2
sd = 0.5

# take the first three
U1 = unif_seq[1:(length(unif_seq)/2)]
# take the remaining
U2 = unif_seq[! unif_seq %in% U1]

# transformation
Z1 = sqrt(-2 * log(U1)) * cos(2 * pi * U2)
Z2 = sqrt(-2 * log(U1)) * sin(2 * pi * U2)
# combine
Z = c(Z1, Z2)

# scale them from standard normal
# multiply by sd means that the variance is sd^2
Z = Z*sd + mean

```

### c)

```{r}
rnorm_zx81 = function(seed, n, mean=0, sd=1) {
  # handle the even and odd thing
  if (n%%2 == 0) {
    U = runif_zx81(seed=seed, n=n)
  } else(
    U = runif_zx81(seed=seed, n=n+1)
  )
  
  # split the draw in half
  U1 = U[1:(length(U)/2)]
  U2 = U[! U %in% U1]
  
  # transformation
  Z1 = sqrt(-2 * log(U1)) * cos(2 * pi * U2)
  Z2 = sqrt(-2 * log(U1)) * sin(2 * pi * U2)
  
  # combine them
  Z = c(Z1, Z2)
  # scale
  Z = Z*sd + mean
  
  return (Z[1:n])
}
```

try it out:

```{r}
mynormals = rnorm_zx81(seed=42, n=1000)

# get a tibble
mynormals_df = mutate(tibble(mynormals), n=row_number())
```

View a histogram: this does look like a slightly skewed normal density

```{r}
hist(mynormals)
```

View a QQ plot: roughly 45-degree line

```{r}
# use the stats package one
qqnorm(mynormals)
qqline(mynormals) 
```

View a time series plot: values are centered at 0

```{r}
ggplot(mynormals_df, aes(x=n, y=mynormals)) +
  geom_line()
```

These look like the data is standard normal

## 3.

```{r}
draw1000_again = runif_zx81(seed=42, n=1000, a=66, m=401)

# take the odd elements
odds <- draw1000_again[seq(1, length(draw1000_again), by=2)]
# take the even elements
evens <- draw1000_again[seq(2, length(draw1000_again), by=2)]

plot(odds, evens)
```

The pattern doesn't look random since they're all lying on a bunch of parallel lines. This corresponds to the result in (Marsaglia, 1968) when applied to 2-dimensional space.

# Football

## 1.

First, we see that H == 1 (games where team A is the home team) have a higher mean SPREAD. When H == -1 (games where team B is the home team) have a lower mean SPREAD (so team B scores). Since SPREAD comes from the difference between team A's score and team B's score, we see that team A scores relative more when they are the home team (and vice versa). This shows that there is a home game advantage.

```{r}
# home team advantage
football |>
  # look at value of SPREAD for H == 1 vs H == -1
  group_by(H) |>
  summarise(mean_SPREAD = mean(SPREAD, na.rm = TRUE))
```

Test of sample means:

```{r}
# subset of data
spread_home_A = football$SPREAD[football$H == 1]
spread_home_B = football$SPREAD[football$H == -1]

# test of sample means
t.test(spread_home_A, spread_home_B, 
       alternative='two.sided', var.equal=FALSE)
```

## 2.

It's because SPREAD is a difference between team A and team B and you're not expectng there to be a difference between the two teams when neither one is doing a home game

## 3.

We reject the null hypothesis that the true coefficient for H is zero. The coefficient is interpreted as the increase in SPREAD for a one-unit increase in H, so as H goes from a team B home game (where team B has the advantage), to a nobody home game (where no one has the advantage), to a team A home game (where team A has the advantage), team A scores relatively more. Going from H == -1 to H == 1 is a two unit increase in H and represents going from a team B home game to a team A home game.

```{r}
# use H to predict SPREAD with no intercept
m3 = lm(SPREAD ~ 0 + H, data = football)

tidy(m3)
```

## 4.

In the pairs plot, all the different types of ranking system differences and the win-loss record are correlated with each other. The correlations all look linear.

```{r}
# pairs plot
pairs(football[c(computer_rankings, 'REC')])
```

It's easier to see these things with the plot of correlations. We can see that MAT, SAG, and MAS are very highly correlated with each other

```{r}
cor(football[, c(computer_rankings, 'REC')])
```

## 5.

Test the null hypothesis that the coefficient on MAT, SAG, BIL, COL, MAS, or DUN is zero at the 5% level. In the model with everything, we can see that the coefficients of MAT, COL, and MAS are not significant. I feel like to properly do this question there are way too many possible orderings to remove the predictors in. If you remove them one by one, different predictors may become non-significant in those regressions. I'm just removing them in order of largest P-value. I'm eventually going with the model that uses H, REC, SAG, BIL, COL and DUN.

Now that I'm handing it in soon it would've been so much easier to do an F test but I did not!!

```{r}
# use H, REC, and all computer predictors to predict SPREAD with no intercept
m5 = lm(SPREAD ~ 0 + H + REC + MAT + SAG + BIL + COL + MAS + DUN, 
                     data=football)

tidy(m5)

```

Removing MAS...

```{r}
# use H, REC, and all computer predictors to predict SPREAD with no intercept
# remove MAS
m5rMAS = lm(SPREAD ~ 0 + H + REC + MAT + SAG + BIL + COL + DUN, 
                      data=football)

tidy(m5rMAS)

```

Removing MAS and MAT...

```{r}
# use H, REC, and all computer predictors to predict SPREAD with no intercept
# remive MAS and MAT
m5rMASMAT = lm(SPREAD ~ 0 + H + REC + SAG + BIL + COL + DUN, 
                         data=football)

tidy(m5rMASMAT)

```

Finding the best individual predictor: want the best (lowest) MSE predictor

I'm about to submit this and I actually have no idea why I went with MSE, I think R-squared would have been fine

```{r}
# save MSEs
MSE_values = c()
for (i in computer_rankings) {
  data = football[c('SPREAD', i)]
  # use the string but as an argument when it's not a string u know what i mean?
  model = lm(as.formula(paste0('SPREAD ~ 0 + ', i)) , data=data)
  # add to all MSE
  MSE_values = c(MSE_values, get_MSE(model))
}

print(tibble(ranking=computer_rankings, MSE=MSE_values))
```

Now, compare this to the model I chose: you can get a lower MSE using H, REC, SAG, BIL, COL, and DUN compared to any of the six individual predictors

```{r}
print(min(MSE_values) > get_MSE(m5rMASMAT))
```

## 6.

Include REC, DUN, COL, BIL, and one of MAT/SAG/MAS since those three are very highly correlated. The regression coefficient on LV is the only one significantly different from zero at the 5% level

```{r}
m6 = lm(SPREAD ~ 0 + H + LV + MAT + REC + BIL + COL + DUN, 
                         data=football)

tidy(m6)

```

## 7.

LV is an amount that would balance the quantity of bets for and against a given team. If people are betting using all information given to them, all of the other stuff should already be incorporated into LV. No other variables should help you predict SPREAD beyond just adding LV. The coefficient on LV should be 1 or else you'd be able to profitably deviate from other bets by betting more aggressively with/against the direction of LV.

```{r}
m7 = lm(SPREAD ~ 0 + LV, data=football)

tidy(m7)

```

Test of the null hypothesis that the true coefficient on LV is 1 at the 5% significance level:

```{r}

# the test statistic

# get the estimated coefficient
bhat_LV = coef(m7)['LV']
# get the standard error
se_LV = summary(m7)$coefficients['LV', 'Std. Error']
# calculate the test statistic
t_LV = (bhat_LV - 1) / se_LV

# the critical value
# get the degrees of freedom
df_LV = summary(m7)$df[2]
criticalvalue_alpha = qt((1 - alpha/2), df_LV)

# reject Ho? 
print(abs(t_LV) > criticalvalue_alpha)
```

We fail to reject the null hypothesis and conclude that the true coefficient on LV is 1

## 8.

```{r warning=FALSE}

stargazer(m3, m5, m5rMAS, m5rMASMAT, m6, m7,
          type='text', 
          dep.var.labels='SPREAD',
          no.space=TRUE,
          digits=3,
          notes=c('* p<0.05; ** p<0.01; *** p<0.001'),
          omit.stat=c('f', 'ser'))
```

# Wells

## 1.

### b), d) and e)

```{r}
wells = wells |>
  # create log arsenic level
  # since arsenic > 0.5 we don't have to worry about taking log(0) of anything
  mutate(larsenic = log(arsenic)) |>
  # create 
  mutate(dist100 = dist / 100) |>
  mutate(zeduc = (educ - mean(educ, na.rm=TRUE)) / sd(educ, na.rm = TRUE))

```

### c)

Histogram of arsenic levels

```{r}
# not log
ggplot(wells, aes(x=arsenic)) +
  geom_histogram(bins=30, fill='hotpink', color='purple') +
  labs(
    title='Distribution of respondent arsenic levels in wells',
    x='Arsenic Level (100s of micrograms per liter)',
    y='Number of observations'
)
```

Histogram of log arsenic levels

```{r}
# log
ggplot(wells, aes(x=larsenic)) +
  geom_histogram(bins=30, fill='green', color='orange') +
  labs(
    title='Distribution of respondent arsenic levels in wells (log)',
    x='Log Arsenic Level (100s of micrograms per liter)',
    y='Number of observations'
)
```

The distribution is very right-skewed (although this doesn't show as much on the log scale since the log values are closer together). Most wells in the data are close to the safe/unsafe threshold.

## 2.

### a)

```{r}
# regression of switch on dist100
fit1 = glm(switch ~ dist100, data=wells, family='binomial')
```

### b)

Logistic regression function for fit1

```{r}
ggplot(wells, aes(x=dist100, y=switch)) +
  # regression line
  stat_smooth(method='glm', method.args=list(family='binomial')) +
  geom_jitter(width = 0.5, # noise in x-coordinate
              height = 0.1) # noise in y-coordinate 
```

### c)

```{r}
summary(fit1)
```

Increase in distance from the nearest safe well decreases log odds of changing the well that you use. This makes sense because if you live really far away from the safe well it is harder for you to switch, so the odds ratio decreases. This is significant at the 0.1% level.

### d)

```{r}
# my function that basically does everything lol
logit_effects_fit1 = logit_effects(fit1, 'dist100')
```

Predicted probability of the average

```{r}
# find the average distance from a safe well
mean_dist100 = logit_effects_fit1$mean_pred

# with type response to get probability
p_switch_meandist_fit1 = logit_effects_fit1$probability_at_mean

print(paste0('p(average household switches) = ', p_switch_meandist_fit1))
```

### e)

```{r}

# logit coefficient
bhat_dist100_fit1 = logit_effects_fit1$coefficient

# marginal effect of average person
marginal_effect_avg_dist100_fit1 = logit_effects_fit1$marginal_effect_average
print(paste0('marginal effect at average: ', marginal_effect_avg_dist100_fit1))

# estimated coefficient / 4
print(paste0('divide-by-four rule: ', bhat_dist100_fit1 / 4))

# average of individual marginal effects
# predict for everyone
p_switch_all_fit1 = logit_effects_fit1$predicted_probabilities
# average marginal effect
avg_partial_effect_fit1 = logit_effects_fit1$average_partial_effect
print(paste0('average partial effect: ', avg_partial_effect_fit1))

```

## 3.

### a) and b)

```{r}

wells = wells |>
  # probabilities
  mutate(p1 = p_switch_all_fit1) |>
  # make the prediction based on the probabilities
  mutate(pred1 = ifelse(p1 > 0.5, 1, 0)) |>
  # check where the prediction is wrong
  mutate(pred_wrong_fit1 = ifelse(pred1 != switch, 1, 0))

```

### c)

```{r}
# the proportion of predictions that are wrong
wells |>
  summarise(proportion_wrong=mean(pred_wrong_fit1))
```

### d)

```{r}
confusion_fit1 = table(pred1 = wells$pred1, switch = wells$switch)
print(confusion_fit1)
```

### e)

sense and sensibility

```{r}

n_TP_fit1 = confusion_fit1['1', '1']
n_TN_fit1 = confusion_fit1['0', '0']
n_FP_fit1 = confusion_fit1['1', '0']
n_FN_fit1 = confusion_fit1['0', '1']

# sensitivity = true positives / (true positives + false negatives)
sensitivity_fit1 = n_TP_fit1 / (n_TP_fit1 + n_FN_fit1)
print(paste0('sensitivity (fit1): ', sensitivity_fit1))

# specificity = true negatives / (true negatives + false positives)
specificity_fit1 = n_TN_fit1 / (n_TN_fit1 + n_FP_fit1)
print(paste0('specificity (fit1): ', specificity_fit1))
```

### f)

The null model:

```{r}
# null error rate from guessing the most common one
switch_values = table(wells$switch)
print(switch_values)
```

More observations have switch == 1 than switch == 2. The null error rate would be...

```{r}

null_error = switch_values['1'] / (switch_values['1'] + switch_values['0'])
print(paste0('null error rate: ', null_error))

```

The regression model has a lower error rate than the null model. The model has high sensitivity, so it's good at predicting when someone does switch wells but has low specificity so is worse at predicting when someone does not switch. Since the null model always predicts switch == 1, there are no false negatives or true negatives. Its sensitivity and specificity are one and zero, respectively.

## 4.

```{r warning=FALSE}

fit2 = glm(switch ~ larsenic, data=wells, family='binomial')
fit3 = glm(switch ~ zeduc, data=wells, family='binomial')
fit4 = glm(switch ~ dist100 + larsenic + zeduc, data=wells, family='binomial')

stargazer(fit1, fit2, fit3, fit4,
          type='text', 
          dep.var.labels='SWITCH',
          no.space=TRUE,
          digits=3,
          notes=c('* p<0.05; ** p<0.01; *** p<0.001'),
          omit.stat=c('f', 'ser'))

```

## 5.

### a)

Logistic regression function for fit2

```{r}
ggplot(wells, aes(x=dist100, y=switch)) +
  # regression line
  stat_smooth(method='glm', method.args=list(family='binomial')) +
  geom_jitter(width = 0.5, # noise in x-coordinate
              height = 0.1) # noise in y-coordinate 
```

```{r}
summary(fit2)
```

Increase in log arsenic level above the safe threshold increases log odds of changing the well that you use. This makes sense because higher arsenic levels (and hence higher log arsenic levels since it is an increasing transformation) are and probably are perceived to be more threatening to the well users and they'd be more likely to switch (less likely to stay using the same one). The farther the level of arsenic rises above the safe level, the odds of switching to safe ones increase. This is significant at the 0.1% level.

### b)

Logistic regression function for fit3

```{r}
ggplot(wells, aes(x=zeduc, y=switch)) +
  # regression line
  stat_smooth(method='glm', method.args=list(family='binomial')) +
  geom_jitter(width = 0.5, # noise in x-coordinate
              height = 0.1) # noise in y-coordinate 
```

```{r}
summary(fit3)
```

Increase in a year of education level of the head of the household increases log odds of changing the well that you use. If they have more education, they are more likely to learn about the dangers of arsenic contamination to health and get their families to change to a safe well. This is significant at the 0.1% level.

### c)

```{r}
# my function that basically does everything
logit_effects_fit4_dist = logit_effects(fit4, 'dist100')
logit_effects_fit4_arse = logit_effects(fit4, 'larsenic')
logit_effects_fit4_educ = logit_effects(fit4, 'zeduc')
```

Marginal effect of average household

```{r}
print(paste0('marginal effect at average (dist100): ', 
             logit_effects_fit4_dist$marginal_effect_average))
print(paste0('marginal effect at average (larsenic): ', 
             logit_effects_fit4_arse$marginal_effect_average))
print(paste0('marginal effect at average (zeduc): ', 
             logit_effects_fit4_educ$marginal_effect_average))
```

A 100-metre increase in distance from a safe well is associated with a 23.9 percentage point decrease in the average household's probability of switching wells. A 1-unit increase in level of arsenic is associated with a 21.7 percentage point increase in the average household's probability of switching wells. A 1-unit increase in years of education of the head of the household is associated with a 4.2 percentage point increase in the average household's probability of switching wells.

Divide-by-4

```{r}
print(paste0('divide-by-four rule (dist100): ', 
             logit_effects_fit4_dist$divide_by_4))
print(paste0('divide-by-four rule (larsenic): ', 
             logit_effects_fit4_arse$divide_by_4))
print(paste0('divide-by-four rule (zeduc): ', 
             logit_effects_fit4_educ$divide_by_4))
```

At the average household, marginal effects are pretty close to the maximum partial effect but slightly lower.

## 6.

Get the predictions again

```{r}
wells = wells |>
  # probabilities 
  # it's the same one for all three of them since it comes from the same fit
  mutate(p4 = logit_effects_fit4_dist$predicted_probabilities) |>
  # make the prediction based on the probabilities
  mutate(pred4 = ifelse(p4 > 0.5, 1, 0)) |>
  # check where the prediction is wrong
  mutate(pred_wrong_fit4 = ifelse(pred4 != switch, 1, 0))
```

Accuracy:

```{r}
# the proportion of predictions that are wrong
wells |>
  summarise(proportion_wrong=mean(pred_wrong_fit4))
```

The confusion matrix:

```{r}
confusion_fit4 = table(pred4 = wells$pred4, switch = wells$switch)
print(confusion_fit4)
```

Calculating sensitivity and specificity:

```{r}

n_TP_fit4 = confusion_fit4['1', '1']
n_TN_fit4 = confusion_fit4['0', '0']
n_FP_fit4 = confusion_fit4['1', '0']
n_FN_fit4 = confusion_fit4['0', '1']

# sensitivity = true positives / (true positives + false negatives)
sensitivity_fit4 = n_TP_fit4 / (n_TP_fit4 + n_FN_fit4)
print(paste0('sensitivity (fit4): ', sensitivity_fit4))

# specificity = true negatives / (true negatives + false positives)
specificity_fit4 = n_TN_fit4 / (n_TN_fit4 + n_FP_fit4)
print(paste0('specificity (fit4): ', specificity_fit4))
```

Recall the null model:

```{r}
print(paste0('null error rate: ', null_error))
```

Recall fit1:

```{r}
print(paste0('sensitivity (fit1): ', sensitivity_fit1))
print(paste0('specificity (fit1): ', specificity_fit1))
```

This model has a better error rate than the null model and fit1. It has a lower sensitivity and a higher specificity than fit1, so it's relatively better at predicting a switcher (compared to fit1).

# NSW

Load the data

```{r}
experimental = cleanup(nsw_dw)
cps_controls = cleanup(controls)
```

Some lists of variables

```{r}
# the character ones i just created
string_variables = c('race', 'str_treat', 'str_nodegree', 'str_married', 
                     'str_employment74', 'str_employment75')
# numerical variables (including dummies)
numerical_variables = c('age', 'education', 'black', 'hispanic', 'treat', 
                        'nodegree', 'married', 'employment74', 'employment75',
                        'earnings74', 'earnings75', 'earnings78')

```

## 2.

### a)

String variables:

```{r}
datasummary_skim(experimental[string_variables])
```

Numerical variables:

```{r}
datasummary_skim(experimental[numerical_variables])
```

### b)

```{r}
datasummary_balance(~ treat, data=experimental[numerical_variables])
```

### c)

```{r}
# earnings in each group
earnings78_0 = experimental$earnings78[experimental$treat == 0]
earnings78_1 = experimental$earnings78[experimental$treat == 1]

# standard error of difference in means
test_of_means = t.test(earnings78_1, earnings78_0, 
                       alternative='two.sided', var.equal=FALSE)
print(test_of_means$conf.int)
```

## 3.

```{r}
# concat
composite = bind_rows(cps_controls, experimental[experimental$treat == 1, ])

datasummary_balance(~ treat, data=composite[numerical_variables])
```

The control group in the CPS data has higher mean earnings78 than the control group in the experimental data. When comparing the treatment group to the CPS controls, the difference in mean earnings78 is not significant at the 5% level.

```{r}
# earnings in each group
earnings78_cps = composite$earnings78[composite$treat == 0]
earnings78_exp = composite$earnings78[composite$treat == 1]

# standard error of difference in means
test_of_means_composite = t.test(earnings78_exp, earnings78_cps, 
                                 alternative='two.sided', var.equal=FALSE)
print(test_of_means_composite$conf.int)
```

## 4.

### a)

```{r}
# regression on all variables
regression = lm(earnings78 ~ treat + age + education + black + hispanic + 
                  married + nodegree + earnings74 + earnings75, data=composite)

tidy(regression)
```

Receiving the treatment increases earnings

### b)

The treatment must be as-good-as randomly assigned after adjusting for covariates. This is the part that allows us to isolate the impact of the treatment itself outside of what other factors might impact outcomes. For any values of the covariates, the treated and untreated groups must have some overlap at those values. This is the part that allows us to actually estimate the CATE for all observations.

### c)

So to actually do it you need to interact the variable treat with all the other covariates or subtract the mean from all the covariates, running the same regression but with the re-parameterised model. In this regression, the estimated coefficient on treat will be the estimated ATE, which in general is not equal to the estimated coefficient on treat in the regression from part (a).

## 5.

### a)

Don't include earnings78 since that's the outcome variable

```{r}
# estimate logit regression of treat
estimate_propensities = glm(treat ~  age + I(age^2) + I(age^3) + education + 
                              I(education^2) + married + nodegree + black + 
                              hispanic + earnings74 + earnings75 + 
                              employment74 + employment75, 
                            data=composite, 
                            family='binomial')
```

### b)
```{r}
# predicted probabilities
composite$propensity = augment(estimate_propensities, 
                               type.predict='response')$.fitted
```

Propensity scores for untreated
```{r}
propensities_0 = composite$propensity[composite$treat == 0]

hist(propensities_0, breaks=100)
```

Propensity scores for treated
```{r}
propensities_1 = composite$propensity[composite$treat == 1]

hist(propensities_1, breaks=100)
```

The untreated group tends to have very low propensity scores while the propensity scores of the treated group are a bit more uniform. These two groups must be pretty different in the included characteristics to provide these differences in propensity scores.

The untreated propensities are also much more centered around the mean, so they're more similar to each other than the treated group are.
```{r}
# 
print(sd(propensities_1))
print(sd(propensities_0))
```

```{r}
t.test(propensities_1, propensities_0, 
       alternative='two.sided', var.equal=FALSE)
```

### c)
```{r}
# calculate
composite = composite |> 
  # calculate weights
  mutate(weight1 = treat / propensity, 
         weight0 = (1 - treat) / (1 - propensity))
```

Average treatment effect is very large and negative
```{r}
composite |>
  summarize(mean(weight1 * earnings78) - mean(weight0 * earnings78)) |>
  pull()

```
### d)
Removing any observations with a propensity score less than 0.1 or greater than 0.9 before calculating the propensity score weighting estimator
```{r}
composite_exclude = composite |>
  # take out these
  filter(propensity >= 0.1, propensity <= 0.9) |>
  # then calculate weights
  mutate(weight1 = treat / propensity, 
         weight0 = (1 - treat) / (1 - propensity))
  
```

Average treatment effect is now not that large and positive
```{r}
composite_exclude |>
  summarize(mean(weight1 * earnings78) - mean(weight0 * earnings78)) |>
  pull()

```

### e)
We removed observations who are very unlikely or very likely to receive treatment based on their observable characteristics. These would have very large weights because the propensity score shows up in the denominator and I guess they had a disproportionate impact on the treatment effect compared to the other observations. 
