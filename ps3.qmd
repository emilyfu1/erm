---
title: "ps3"
author: "1088708"
date: "`r Sys.Date()`"
format: html
editor: visual
---

```{r message=FALSE}
library(tidyverse)
library(stargazer)
library(broom)
library(ivreg)
library(MASS)
library(purrr)

# import data
weber = read_csv('https://ditraglia.com/data/weber.csv')

# set parameters
n = 100
sims_combinations = expand_grid(rho = c(0.5, 0.9, 0.99), 
                                conc = c(0, 0.25, 10, 100))

```

## 1. Weber

### a)
They try to explain the relative economic prosperity of Protestant regions compared to others, proposing an alternative mechanism for this phenomenon to Weber's protestant work ethic hypothesis. They say that because Martin Luther favoured universal schooling, this educational attainment was then useful for economic activity.

### b)
Weber wasn't necessarily wrong in an observational way, since BW do observe a significant, positive association between Protestantism and economic prosperity in late-nineteenth-century Prussia. However, instead of saying that Protestant theology causes protestants to work harder, BW argues that they're more successful because they were not Jared (19) and also because Martin Luther produced the first widely used German translation of the Bible instead of wanting it to only be read out in Latin which many people did not understand. Since Protestantism spread around Martin Luther's city of Wittenberg, BW use distance from Wittenberg as an instrument for Protestantism to identify the impact of Protestantism on literacy.

### c)
Relevance: Tee instrument must be correlated with the explanatory variable. The distance from Wittenburg must be related to literacy,

Exogeneity: The instrument must be uncorrelated with the error term. Distance is a

First, nineteenth-century
Prussia has the birthplace of the Reformation at its center. Luther
proclaimed his 95 Theses in Wittenberg, and the Prussian territory conserved Protestantism in its purest form. Second, Prussia
is Max Weber’s birthplace, and his views were shaped by what he
observed across Germany. Third, Prussia had rather uniform laws
and institutional frameworks, with the possible exception of recent annexations (dealt with below). By contrast, cross-country
comparisons, which constitute the existing literature, are notoriously plagued by the difficulty of netting out the effects of
other fundamental causes, such as institutions and geography

## 2.

### a)

```{r warning=FALSE}
# regress f_rw on f_prot 
ols = lm(f_rw ~ f_prot, data=weber)
```

### b)
```{r warning=FALSE}
# display 
stargazer(ols,
          type='text', 
          dep.var.labels='Literacy rate',
          no.space=TRUE,
          digits=3,
          notes=c('* p<0.05; ** p<0.01; *** p<0.001'),
          omit.stat=c('f', 'ser'))
```

### c)
Choice of education 

When education became more widespread in subsequent
centuries, these regions could have more easily afforded to educate their children. The fact that “Protestantism” was initially a “protest” movement involving peasant uprisings that reflected
social discontent is suggestive of such a negative selection bias


## 3.

### a)

Yes, kmwittenberg does appear to be a relevant instrument for f_prot

```{r}
# first-stage regression of f_prot on kmwittenberg
first_stage = lm(f_prot ~ kmwittenberg, data=weber)

tidy(first_stage)
```

### b)
```{r}
# reduced-form regression of f_rw on kmwittenberg
reduced_form = lm(f_rw ~ kmwittenberg, data=weber)

tidy(first_stage)
```

### c) and d)
```{r}
# IV regression of f_rw on f_prot using kmwittenberg as an instrument for f_prot
iv = ivreg(f_rw ~ f_prot | kmwittenberg, data = weber)

tidy(iv)
```

### e)
```{r}
# the just-identified case: indirect least squares
bhat_2sls = coef(reduced_form)['kmwittenberg'] / coef(first_stage)['kmwittenberg']

# check if coefficients are equal

# what the heck
print(bhat_2sls[['kmwittenberg']])
print(coef(iv)[['f_prot']])
print(bhat_2sls[['kmwittenberg']]  == coef(iv)[['f_prot']])
```

## 4.

### a)
```{r}
ols_full = lm(f_rw ~ f_prot + f_young + f_jew + f_fem + 
                f_ortsgeb + f_pruss + hhsize + lnpop + 
                gpop + f_miss + f_blind + f_deaf + f_dumb, data=weber)

tidy(ols_full)
```

### b)
```{r}
# first-stage regression of f_prot on kmwittenberg and demographic controls
first_stage_full = lm(f_prot ~ kmwittenberg + f_jew + f_fem + 
                        f_ortsgeb + f_pruss + hhsize + lnpop + 
                        gpop + f_miss + f_blind + f_deaf + f_dumb, data=weber)

# IV regression of f_rw on f_prot using kmwittenberg as an instrument for f_prot
# and demographic controls
iv_full = ivreg(f_rw ~ f_jew + f_fem + f_ortsgeb + f_pruss + 
                   hhsize + lnpop + gpop + f_miss + f_blind + 
                   f_deaf + f_dumb + f_prot |
                   f_jew + f_fem + f_ortsgeb + f_pruss + 
                   hhsize + lnpop + gpop + f_miss + f_blind + 
                   f_deaf + f_dumb + kmwittenberg, data = weber)

tidy(first_stage_full)
```

### c)
```{r, warning=FALSE}
stargazer(ols_full, first_stage_full, iv_full,
          type='text', 
          no.space=TRUE,
          digits=3,
          notes=c('* p<0.05; ** p<0.01; *** p<0.001'),
          omit.stat=c('f', 'ser'))
```

### d)
Ihe IV estimated coefficient is higher on f_prot than the OLS coefficient, which suggests the "negative selection bias" of the OLS results. A one percentage point increase in the percentage of Protestants is estimated to increase the literacy rate by 0.190 percentage points, which is higher than the OLS estimate.

# Weak IV

## 1. 
```{r}
z_fixed = rnorm(n)

# scale to sample mean
z_fixed = z_fixed - mean(z_fixed)

# scale to sum of squares
z_fixed = z_fixed * sqrt(n / sum(z_fixed^2))
```

## 2. 
```{r}
draw_sim_data = function(pi, rho, z, n=100, beta=0) {
  
  # joint distribution of u and v
  sigma = matrix(c(1, rho, rho, 1), nrow=2)
  
  # jointly generate u and v
  u_v = mvrnorm(n=n, mu=c(0, 0), Sigma=sigma)
  u = u_v[, 1]
  v = u_v[, 2]
  
  # get x and y
  x = pi * z + v
  y = beta * x + u
  
  # form dataframe
  data = tibble(x=x, y=y, z=z)
  
  return(data)
}
```

## 3.
```{r}
get_iv_stats = function(dat) {
  
  # get first stage and reduced form
  first_stage = lm(x ~ 0 + z, data=dat)
  reduced_form = lm(y ~ 0 + z, data=dat)
  
  # the columns
  y = dat$y
  x = dat$x
  z = dat$z
  
  # calculate the estimated coefficient
  bhat_2sls = coef(reduced_form)['z'] / coef(first_stage)['z']

  # the estimated residuals
  uhat = y - bhat_2sls * x

  # estimated residual variance
  sigma2_uhat = sum(uhat^2) / (n - 2)

  # estimated standard error of bhat 
  se_bhat = sqrt(sigma2_uhat * sum(z^2) / (sum(z * x)^2))

  return(c(est = bhat_2sls, se = se_bhat))
}
```

## 4.
Using the function
```{r}
draws = draw_sim_data(pi=1, rho=0.5, z=z_fixed)
estimates = get_iv_stats(dat=draws)
print(estimates)
```

Using ivreg, we get the same estimated coefficient and standard error!
```{r}
imrunningoutofwordstocallthesethings = ivreg(y ~ 0 + x | 0 + z, data = draws)
summary(imrunningoutofwordstocallthesethings)
```

## 5. 
```{r}
replicate_iv_sim = function(n_reps, conc, rho, z) {
  
  # find the first stage from the concentration parameter
  pi = sqrt(conc / sum(z^2))
  
  # store results to a dataframe
  sims = tibble(est=numeric(), 
                se=numeric(), 
                conc=numeric(), 
                rho=numeric())
  
  # repeat draws
  for (rep in 1:n_reps) {
    
    
    draws = draw_sim_data(pi=pi, rho=rho, z=z)
    
    iv_results = get_iv_stats(draws)
    
    # append results to a dataframe    
    new_row = tibble(est=iv_results['est.z'], 
                     se=iv_results['se'], 
                     conc=conc, 
                     rho=rho)
    sims = bind_rows(sims, new_row)    
  }
  
  # return results
  return (sims)
}
```

## 6.
Replications
```{r}
simulations = pmap_dfr(.l = list(conc=sims_combinations$conc, 
                                 rho=sims_combinations$rho), 
                       .f = function(conc, rho) {replicate_iv_sim(n_reps=10000, 
                                                                  conc=conc, 
                                                                  rho=rho, 
                                                                  z=z_fixed)})
```

Table of median bias compared to true parameter value
```{r}
simulations |>
  group_by(rho, conc)|>
  summarise(median_est=median(est), .groups='drop')
```

I'm noticing that the simulations with higher concentrations (stronger instrumental variables) have median estimates closer to zero. This makes sense since when the instrument is less weak it will be less biased in finite samples and better reflects the true relationship. Simulations with higher covariance between U and V have median estimates that are further from zero (in magnitude). If there is correlation with U and V, then X is correlated with U due to the DGP for X.

## 7.
First remove extreme values:
```{r}
filter_simulations = simulations |>
  filter(est >= -3, est <= 3)
```

Kernel density plots:
```{r}
ggplot(filter_simulations, aes(x=est, colour=factor(conc))) +
  geom_density() + 
  facet_wrap(~ rho) + # different values of rho
  labs(title = 'densities of the 2SLS estimator by concentration', 
       x = 'estimated coefficient', 
       color='concentration') +
  theme_minimal()

ggplot(filter_simulations, aes(x=est/se, colour=factor(conc))) +
  geom_density() + 
  facet_wrap(~ rho) + # different values of rho
  labs(title = 'densities of the 2SLS estimator t statistic by concentration', 
       x = 't statistic', 
       color='concentration') +
  theme_minimal()
```
In the plots, I see that the simulations with stronger instruments are centered around the true value of beta based on the DGP, while the simulations with weaker instruments are further away from the true value (but are closer to zero as concentration increases), which is what we can also see from the table. In the plot of the test statistics, I'm noticing that the plot for the regressions with the highest covariance between U and V is more spread out around zero, but this is more noticeable for lower concentrations.

# MLDA

# my mini project
